# scrape_skyliautie_fixed.py
import os
import re
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

BASE_URL = "https://sites.google.com/site/skyliautie/"
LIST_URL = "https://sites.google.com/site/skyliautie/shi/lech"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
}

def sanitize_filename(title: str) -> str:
    title = title.strip().lower()
    title = re.sub(r'\s+', '_', title)
    title = re.sub(r'[^\w\-]', '', title)
    return title if title else "untitled"

def extract_riparline_poem(soup: BeautifulSoup) -> str:
    """
    2025å¹´ç¾åœ¨ã®Skyliautieè©©ãƒšãƒ¼ã‚¸ã‹ã‚‰ã€ãƒªãƒ‘ãƒ©ã‚¤ãƒ³èªã®è©©éƒ¨åˆ†ã ã‘ã‚’æ­£ç¢ºã«æŠ½å‡º
    """
    # æ–¹æ³•1: <pre>ã‚¿ã‚°ãŒã‚ã‚Œã°æœ€å„ªå…ˆï¼ˆå¤ã„ãƒšãƒ¼ã‚¸ç”¨ï¼‰
    pre = soup.find("pre")
    if pre and pre.get_text(strip=True):
        return pre_text = pre.get_text(separator="\n")
        if any(c in "Ã¡Ã©Ã­Ã³ÃºÃ¤Ã«Ã¯Ã¶Ã¼Ã£ÃµÃ±Ã§'â€™" for c in pre_text):
            return pre_text.strip()

    # æ–¹æ³•2: Courier New ãªã©ã®ç­‰å¹…ãƒ•ã‚©ãƒ³ãƒˆã§æ›¸ã‹ã‚ŒãŸãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¢ã™ï¼ˆã“ã‚ŒãŒç¾åœ¨ä¸»æµï¼‰
    candidates = []
    for elem in soup.find_all(["div", "span", "p"], style=True):
        style = elem.get("style", "")
        if "courier" in style.lower() or "monospace" in style.lower() or "font-family:'courier new'" in style.lower():
            text = elem.get_text(separator="\n")
            if len(text) > 50:  # è©©ã¯ãã‚Œãªã‚Šã«é•·ã„
                candidates.append(text)

    # æ–¹æ³•3: classåã« "sites-canvas-main-content" å†…ã®é•·ã„ãƒ†ã‚­ã‚¹ãƒˆãƒ–ãƒ­ãƒƒã‚¯ãªã©
    if not candidates:
        main = soup.find("div", class_=re.compile(r"sites-canvas-main-content|main-content", re.I))
        if main:
            text = main.get_text(separator="\n")
            lines = [line.strip() for line in text.splitlines() if line.strip()]
            # ãƒªãƒ‘ãƒ©ã‚¤ãƒ³èªã£ã½ã„è¡Œã‚’å„ªå…ˆæ¡å–
            rip_lines = [ln for ln in lines if re.search(r"[Ã¡Ã©Ã­Ã³ÃºÃ½Ã¤Ã«Ã¯Ã¶Ã¼Ã£ÃµÃ±Ã§']", ln) and not re.search(r'[\u3000-\u303F\u3040-\u309F\u30A0-\u30FF\uFF00-\uFFEF]', ln)]
            if rip_lines:
                # é€£ç¶šã—ã¦ã„ã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¢ã™
                blocks = []
                curr = []
                for line in lines:
                    if re.search(r"[Ã¡Ã©Ã­Ã³ÃºÃ½Ã¤Ã«Ã¯Ã¶Ã¼Ã£ÃµÃ±Ã§']", line) and len(line) > 5:
                        curr.append(line)
                    elif curr:
                        blocks.append("\n".join(curr))
                        curr = []
                if curr:
                    blocks.append("\n".join(curr))
                if blocks:
                    # ä¸€ç•ªé•·ã„ãƒ–ãƒ­ãƒƒã‚¯ãŒé•·ã„ã‚‚ã®ã‚’æ¡ç”¨
                    return max(blocks, key=len).strip()

    # å€™è£œãŒã‚ã‚Œã°ä¸€ç•ªé•·ã„ã‚‚ã®ã‚’è¿”ã™
    if candidates:
        return max(candidates, key=len).strip()

    # æœ€çµ‚æ‰‹æ®µï¼šå…¨ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ—¥æœ¬èªãƒ»è‹±èªã‚’é™¤å¤–ã—ã¦æ®‹ã£ãŸéƒ¨åˆ†
    full_text = soup.get_text(separator="\n")
    lines = []
    for line in full_text.splitlines():
        line = line.strip()
        if not line:
            continue
        # æ—¥æœ¬èªãƒ»é•·ã„è‹±èªãƒ»ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é™¤å¤–
        if re.search(r'[ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾ ]', line):
            continue
        if re.search(r'^[a-zA-Z\s]{15,}$', line):  # é•·ã™ãã‚‹è‹±èªè¡Œ
            continue
        if any(nav in line for nav in ["Google Sites", "Report abuse", "Skyliautie", "ãƒ›ãƒ¼ãƒ "]):
            continue
        lines.append(line)

    return "\n".join(lines).strip() if lines else ""

def get_poem_title(soup: BeautifulSoup, url: str) -> str:
    title_tag = soup.find("title")
    if title_tag and " - Skyliautie" in title_tag.get_text():
        return title_tag.get_text().split(" - Skyliautie")[0].strip()
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šURLã®æœ€å¾Œ
    return os.path.basename(url.split("?")[0])

def get_all_poem_links() -> list:
    r = requests.get(LIST_URL, headers=HEADERS)
    r.raise_for_status()
    soup = BeautifulSoup(r.content, "html.parser")

    links = []
    for a in soup.find_all("a", href=True):
        href = a["href"]
        if "/shi/lech/" in href and not href.endswith("/shi/lech"):
            full_url = urljoin(BASE_URL, href)
            if "?authuser=0" not in full_url:
                full_url += "?authuser=0"
            title = a.get_text(strip=True) or os.path.basename(href)
            links.append((title, full_url))
    return links

def main():
    os.makedirs("riparline_corpus", exist_ok=True)
    os.chdir("riparline_corpus")

    links = get_all_poem_links()
    print(f"æ¤œå‡ºã•ã‚ŒãŸè©©ãƒšãƒ¼ã‚¸æ•°: {len(links)} å€‹")

    success = 0
    for i, (_, url) in enumerate(links, 1):
        print(f"[{i:02d}/{len(links)}] å–å¾—ä¸­ â†’ {url}")
        try:
            r = requests.get(url, headers=HEADERS, timeout=15)
            r.raise_for_status()
            soup = BeautifulSoup(r.content, "html.parser")

            title = get_poem_title(soup, url)
            poem = extract_riparline_poem(soup)

            if not poem or len(poem) < 30:
                print("    âš ï¸  è©©ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆçŸ­ã™ãã‚‹ã‹ç©ºï¼‰")
                continue

            filename_base = f"Skyl.X-Xï¼ˆ{sanitize_filename(title)}ï¼‰.txt"
            filename = filename_base
            counter = 1
            while os.path.exists(filename):
                filename = f"Skyl.X-Xï¼ˆ{sanitize_filename(title)}_{counter}ï¼‰.txt"
                counter += 1

            with open(filename, "w", encoding="utf-8") as f:
                f.write(poem + "\n")

            print(f"    âœ… ä¿å­˜å®Œäº† â†’ {filename} ({len(poem.splitlines())}è¡Œ)")
            success += 1

        except Exception as e:
            print(f"    âŒ ã‚¨ãƒ©ãƒ¼: {e}")

    print(f"\nğŸ‰ å®Œäº†ï¼ {success}/{len(links)} ä»¶ã®è©©ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
    print("   ãƒ•ã‚©ãƒ«ãƒ€: riparline_corpus")

if __name__ == "__main__":
    main()