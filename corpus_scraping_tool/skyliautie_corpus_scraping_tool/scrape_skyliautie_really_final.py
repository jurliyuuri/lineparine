# scrape_skyliautie_really_final.py  â† ä»Šåº¦ã“ãæœ¬å½“ã«æœ€çµ‚ç‰ˆã§ã™ï¼ï¼ï¼
import os
import re
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

BASE_URL = "https://sites.google.com/site/skyliautie/"

CATEGORIES = [
    "https://sites.google.com/site/skyliautie/shi/lech",
    "https://sites.google.com/site/skyliautie/shi/d",
    "https://sites.google.com/site/skyliautie/shi/y",
    "https://sites.google.com/site/skyliautie/shi/k",
]

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
}

def sanitize_filename(title: str) -> str:
    title = title.strip().lower()
    title = re.sub(r'\s+', '_', title)
    title = re.sub(r'[^\w\-]', '', title)
    return title if title else "untitled"

def extract_ban_missen_18lines(soup: BeautifulSoup) -> str:
    full_text = soup.get_text(separator="\n")
    lines = [line.strip() for line in full_text.split("\n") if line.strip()]

    start_idx = -1
    for i, line in enumerate(lines):
        if re.search(r"ban\s+missen\s+tonir.*birleen.*alefis\s+io", line, re.IGNORECASE):
            start_idx = i
            break
    
    if start_idx == -1:
        return ""

    poem_lines = []
    idx = start_idx
    while len(poem_lines) < 18 and idx < len(lines):
        line = lines[idx].strip()
        if line:
            poem_lines.append(line)
        idx += 1

    if len(poem_lines) == 18:
        return "\n".join(poem_lines)
    
    return ""

def get_poem_title(soup: BeautifulSoup, url: str) -> str:
    if soup.title and soup.title.string:
        text = soup.title.string.strip()
        if " - Skyliautie" in text:
            return text.split(" - Skyliautie")[0].strip()
    return os.path.basename(url.split("?")[0])

def get_all_individual_links(category_url: str) -> list:
    """æœ€æ–°Google Siteså¯¾å¿œï¼šã©ã‚“ãªhrefã§ã‚‚/shi/ä»¥ä¸‹ã§å€‹åˆ¥ãƒšãƒ¼ã‚¸ã£ã½ã„ã‚‚ã®ã‚’å…¨éƒ¨å–ã‚‹"""
    try:
        r = requests.get(category_url, headers=HEADERS, timeout=30)
        r.raise_for_status()
        soup = BeautifulSoup(r.content, "html.parser")
        links = []
        
        for a in soup.find_all("a", href=True):
            href = a["href"]
            text = a.get_text(strip=True)
            
            # æ¡ä»¶ã‚’å¤§å¹…ã«ç·©å’Œï¼ˆæœ€æ–°æ§‹é€ å¯¾å¿œï¼‰
            if "/site/skyliautie/shi/" in href or href.startswith("/site/skyliautie/shi/"):
                # ã‚«ãƒ†ã‚´ãƒªãƒˆãƒƒãƒ—è‡ªä½“ã¯é™¤å¤–
                if href.endswith(("/lech", "/d", "/y", "/k")):
                    continue
                full_url = urljoin("https://sites.google.com", href)
                if "?authuser=0" not in full_url:
                    full_url += "?authuser=0"
                    
                title = text if text else os.path.basename(href.split("?")[0])
                links.append((title, full_url))
                
        return links
    except Exception as e:
        print(f"ã‚«ãƒ†ã‚´ãƒªèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {category_url}: {e}")
        return []

def main():
    os.makedirs("riparline_corpus", exist_ok=True)
    os.chdir("riparline_corpus")

    all_links = []
    for cat in CATEGORIES:
        print(f"\nã‚«ãƒ†ã‚´ãƒªèª­ã¿è¾¼ã¿ä¸­: {cat}")
        links = get_all_individual_links(cat)
        print(f"  â†’ {len(links)} ä»¶ã®å€‹åˆ¥ãƒšãƒ¼ã‚¸ã‚’ç™ºè¦‹ï¼")
        all_links.extend(links)

    # é‡è¤‡é™¤å»
    seen = set()
    unique_links = []
    for title, url in all_links:
        if url not in seen:
            seen.add(url)
            unique_links.append((title, url))

    print(f"\nç·ç™ºè¦‹ãƒšãƒ¼ã‚¸æ•°ï¼ˆé‡è¤‡é™¤å»å¾Œï¼‰: {len(unique_links)} ãƒšãƒ¼ã‚¸\n")

    success = 0
    for i, (_, url) in enumerate(unique_links, 1):
        print(f"[{i:03d}/{len(unique_links)}] å‡¦ç†ä¸­ â†’ {url}")
        try:
            r = requests.get(url, headers=HEADERS, timeout=30)
            r.raise_for_status()
            soup = BeautifulSoup(r.content, "html.parser")

            poem = extract_ban_missen_18lines(soup)

            if poem.count("\n") + 1 != 18:
                print("    â†’ ã“ã®ãƒšãƒ¼ã‚¸ã«ã¯Ban missen...ã®18è¡Œè©©ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
                continue

            title = get_poem_title(soup, url)
            safe_title = sanitize_filename(title)
            filename = f"Skyl.X-Xï¼ˆ{safe_title}ï¼‰.txt"

            counter = 1
            while os.path.exists(filename):
                filename = f"Skyl.X-Xï¼ˆ{safe_title}_{counter}ï¼‰.txt"
                counter += 1

            with open(filename, "w", encoding="utf-8") as f:
                f.write(poem + "\n")

            print(f"    âœ… ä¿å­˜å®Œäº† â†’ {filename}")
            success += 1

        except Exception as e:
            print(f"    âŒ ã‚¨ãƒ©ãƒ¼: {e}")

    print(f"\nğŸ‰ğŸ‰ğŸ‰ å¤§æˆåŠŸï¼åˆè¨ˆ {success} ç¯‡ã®ç´”ç²‹ãƒªãƒ‘ãƒ©ã‚¤ãƒ³èªå™äº‹è©©ã‚’ä¿å­˜ã—ã¾ã—ãŸï¼")
    print("   ãƒ•ã‚©ãƒ«ãƒ€: riparline_corpus")
    print("   å½¢å¼: Skyl.X-Xï¼ˆã‚¿ã‚¤ãƒˆãƒ«ï¼‰.txt â† å®Œç’§ã§ã™ï¼ï¼")

if __name__ == "__main__":
    main()